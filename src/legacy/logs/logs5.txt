10/11/2023 21:09:20 - WARNING - __main__ - device: cuda:3, n_gpu: 8, 16-bits training: True
Using pad_token, but it is not set yet.

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.32s/it]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.76s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  2.79s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.11s/it]
10/11/2023 21:10:59 - INFO - __main__ - Namespace(model_type='llama', model_name_or_path='meta-llama/Llama-2-13b-chat-hf', prompt='data3/original_data.txt', length=300, num_hidden_layers=None, stop_token=None, temperature=1.0, repetition_penalty=1.0, k=0, p=0, prefix='', suffix='', padding_text='', xlm_language='', seed=42, data_folder=42, no_cuda=False, stream_output=False, num_return_sequences=1, fp16=True, jit=False, device=device(type='cuda', index=3), n_gpu=8)
10/11/2023 21:10:59 - INFO - Unlimiformer - Encoding 0 to 5 out of 25
10/11/2023 21:10:59 - INFO - Unlimiformer -   Williamson is baking
10/11/2023 21:11:00 - INFO - Unlimiformer - Williamson is baking
10/11/2023 21:11:02 - INFO - Unlimiformer - Encoding 6 to 12 out of 25
10/11/2023 21:11:02 - INFO - Unlimiformer -   Oppenheimer is cycling
10/11/2023 21:11:02 - INFO - Unlimiformer - Oppenheimer is cycling
10/11/2023 21:11:02 - INFO - Unlimiformer - Encoding 13 to 18 out of 25
10/11/2023 21:11:02 - INFO - Unlimiformer -   Leechenbaum is painting
10/11/2023 21:11:03 - INFO - Unlimiformer - Leechenbaum is painting
10/11/2023 21:11:03 - INFO - Unlimiformer - Encoding 19 to 25 out of 25
10/11/2023 21:11:03 - INFO - Unlimiformer -   Zelensky is relaxing
10/11/2023 21:11:04 - INFO - Unlimiformer - Zelensky is relaxing
10/11/2023 21:11:04 - INFO - Unlimiformer - "Pre Forward Hook", <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>[INST] <<SYS>>
You are a helpful assistant. Answer with detailed responses according to the entire instruction or question. 
<</SYS>>

, 63
10/11/2023 21:11:04 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/contrib/torch_utils.py:44: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 2)
10/11/2023 21:11:15 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/11/2023 21:11:22 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/11/2023 21:11:29 - INFO - Unlimiformer - "Pre Forward Hook", above, 1
10/11/2023 21:11:37 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/11/2023 21:11:43 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/11/2023 21:11:50 - INFO - Unlimiformer - "Pre Forward Hook", can, 1
10/11/2023 21:11:57 - INFO - Unlimiformer - "Pre Forward Hook", you, 1
10/11/2023 21:12:05 - INFO - Unlimiformer - "Pre Forward Hook", tell, 1
10/11/2023 21:12:11 - INFO - Unlimiformer - "Pre Forward Hook", me, 1
10/11/2023 21:12:18 - INFO - Unlimiformer - "Pre Forward Hook", if, 1
10/11/2023 21:12:25 - INFO - Unlimiformer - "Pre Forward Hook", Lee, 1
10/11/2023 21:12:32 - INFO - Unlimiformer - "Pre Forward Hook", chen, 1
10/11/2023 21:12:39 - INFO - Unlimiformer - "Pre Forward Hook", baum, 1
10/11/2023 21:12:46 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/11/2023 21:12:53 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/11/2023 21:13:00 - INFO - Unlimiformer - "Pre Forward Hook", ?, 1
10/11/2023 21:13:07 - INFO - Unlimiformer - "Pre Forward Hook", [, 1
10/11/2023 21:13:14 - INFO - Unlimiformer - "Pre Forward Hook", /, 1
10/11/2023 21:13:22 - INFO - Unlimiformer - "Pre Forward Hook", INST, 1
10/11/2023 21:13:29 - INFO - Unlimiformer - "Pre Forward Hook", ], 1
10/11/2023 21:13:36 - INFO - Unlimiformer - "Pre Forward Hook", , 1
10/11/2023 21:13:43 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
10/11/2023 21:13:50 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/11/2023 21:13:57 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/11/2023 21:14:04 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/11/2023 21:14:11 - INFO - Unlimiformer - "Pre Forward Hook", provided, 1
10/11/2023 21:14:18 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/11/2023 21:14:25 - INFO - Unlimiformer - "Pre Forward Hook", it, 1
10/11/2023 21:14:32 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/11/2023 21:14:39 - INFO - Unlimiformer - "Pre Forward Hook", not, 1
10/11/2023 21:14:46 - INFO - Unlimiformer - "Pre Forward Hook", explicitly, 1
10/11/2023 21:14:52 - INFO - Unlimiformer - "Pre Forward Hook", stated, 1
10/11/2023 21:14:59 - INFO - Unlimiformer - "Pre Forward Hook", that, 1
10/11/2023 21:15:06 - INFO - Unlimiformer - "Pre Forward Hook", Lee, 1
10/11/2023 21:15:13 - INFO - Unlimiformer - "Pre Forward Hook", chen, 1
10/11/2023 21:15:20 - INFO - Unlimiformer - "Pre Forward Hook", baum, 1
10/11/2023 21:15:27 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/11/2023 21:15:34 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/11/2023 21:15:41 - INFO - Unlimiformer - "Pre Forward Hook", ., 1
10/11/2023 21:15:48 - INFO - Unlimiformer - "Pre Forward Hook", The, 1
10/11/2023 21:15:55 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/11/2023 21:16:02 - INFO - Unlimiformer - "Pre Forward Hook", mentions, 1
10/11/2023 21:16:09 - INFO - Unlimiformer - "Pre Forward Hook", that, 1
10/11/2023 21:16:15 - INFO - Unlimiformer - "Pre Forward Hook", Lee, 1
10/11/2023 21:16:22 - INFO - Unlimiformer - "Pre Forward Hook", chen, 1
10/11/2023 21:16:30 - INFO - Unlimiformer - "Pre Forward Hook", baum, 1
10/11/2023 21:16:37 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/11/2023 21:16:44 - INFO - Unlimiformer - "Pre Forward Hook", ", 1
10/11/2023 21:16:51 - INFO - Unlimiformer - "Pre Forward Hook", relax, 1
10/11/2023 21:16:58 - INFO - Unlimiformer - "Pre Forward Hook", ing, 1
10/11/2023 21:17:06 - INFO - Unlimiformer - "Pre Forward Hook", ", 1
10/11/2023 21:17:13 - INFO - Unlimiformer - "Pre Forward Hook", and, 1
10/11/2023 21:17:20 - INFO - Unlimiformer - "Pre Forward Hook", ", 1
10/11/2023 21:17:27 - INFO - Unlimiformer - "Pre Forward Hook", en, 1
10/11/2023 21:17:34 - INFO - Unlimiformer - "Pre Forward Hook", jo, 1
10/11/2023 21:17:41 - INFO - Unlimiformer - "Pre Forward Hook", ying, 1
10/11/2023 21:17:48 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/11/2023 21:17:55 - INFO - Unlimiformer - "Pre Forward Hook", sun, 1
10/11/2023 21:18:02 - INFO - Unlimiformer - "Pre Forward Hook", ,", 1
10/11/2023 21:18:10 - INFO - Unlimiformer - "Pre Forward Hook", but, 1
10/11/2023 21:18:17 - INFO - Unlimiformer - "Pre Forward Hook", it, 1
10/11/2023 21:18:24 - INFO - Unlimiformer - "Pre Forward Hook", does, 1
10/11/2023 21:18:31 - INFO - Unlimiformer - "Pre Forward Hook", not, 1
10/11/2023 21:18:38 - INFO - Unlimiformer - "Pre Forward Hook", mention, 1
10/11/2023 21:18:45 - INFO - Unlimiformer - "Pre Forward Hook", any, 1
10/11/2023 21:18:53 - INFO - Unlimiformer - "Pre Forward Hook", specific, 1
10/11/2023 21:19:00 - INFO - Unlimiformer - "Pre Forward Hook", activity, 1
10/11/2023 21:19:07 - INFO - Unlimiformer - "Pre Forward Hook", such, 1
10/11/2023 21:19:14 - INFO - Unlimiformer - "Pre Forward Hook", as, 1
10/11/2023 21:19:22 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/11/2023 21:19:29 - INFO - Unlimiformer - "Pre Forward Hook", ., 1
10/11/2023 21:19:36 - INFO - Unlimiformer - "Pre Forward Hook", Therefore, 1
10/11/2023 21:19:43 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/11/2023 21:19:51 - INFO - Unlimiformer - "Pre Forward Hook", I, 1
10/11/2023 21:19:58 - INFO - Unlimiformer - "Pre Forward Hook", cannot, 1
10/11/2023 21:20:06 - INFO - Unlimiformer - "Pre Forward Hook", confirm, 1
Traceback (most recent call last):
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 583, in <module>
    main()
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 534, in main
    output_sequences = model.generate(
                       ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 670, in pre_generate_hook
    vals_pred.append(self.original_generate_func(input_ids_prefix, **new_kwargs))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 1648, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 2730, in sample
    outputs = self(
              ^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 755, in pre_forward_hook
    result = self.original_forward_func(input_ids=input_ids, labels=labels, attention_mask=attention_mask, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 820, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 708, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 424, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 794, in attention_pre_forward_hook
    result = original_cross_attn_forward_func(hidden_states=hidden_states, attention_mask=attention_mask, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 333, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 187, in apply_rotary_pos_emb
    k_embed = (k * cos) + (rotate_half(k) * sin)
                           ^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 173, in rotate_half
    def rotate_half(x):
    
KeyboardInterrupt
