10/31/2023 22:51:47 - WARNING - __main__ - device: cuda:3, n_gpu: 8, 16-bits training: True
Using pad_token, but it is not set yet.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:32<01:04, 32.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:01<00:30, 30.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:18<00:00, 24.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:18<00:00, 26.16s/it]
10/31/2023 22:57:17 - INFO - __main__ - Namespace(model_type='llama', model_name_or_path='meta-llama/Llama-2-13b-chat-hf', prompt='data_final_data1/original_data.txt', length=300, num_hidden_layers=None, stop_token=None, temperature=1.0, repetition_penalty=1.0, k=0, p=0, prefix='', suffix='', padding_text='', xlm_language='', seed=42, tokens_ind=' 0 2 3 4 5 6 7 8 9 10', data_folder=42, no_cuda=False, stream_output=False, num_return_sequences=1, fp16=True, jit=False, device=device(type='cuda', index=3), n_gpu=8)
Traceback (most recent call last):
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 584, in <module>
    main()
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 476, in main
    'tokens_ind': [int(i) for i in args.tokens_ind.split(" ")]
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 476, in <listcomp>
    'tokens_ind': [int(i) for i in args.tokens_ind.split(" ")]
                   ^^^^^^
ValueError: invalid literal for int() with base 10: ''
