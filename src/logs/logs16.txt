10/12/2023 18:31:12 - WARNING - __main__ - device: cuda:7, n_gpu: 8, 16-bits training: True
Using pad_token, but it is not set yet.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:04<00:08,  4.16s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:08<00:04,  4.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.34s/it]
10/12/2023 18:32:51 - INFO - __main__ - Namespace(model_type='llama', model_name_or_path='meta-llama/Llama-2-13b-chat-hf', prompt='data3/original_data.txt', length=300, num_hidden_layers=None, stop_token=None, temperature=1.0, repetition_penalty=1.0, k=0, p=0, prefix='', suffix='', padding_text='', xlm_language='', seed=42, data_folder=42, no_cuda=False, stream_output=False, num_return_sequences=1, fp16=True, jit=False, device=device(type='cuda', index=7), n_gpu=8)
10/12/2023 18:32:51 - INFO - Unlimiformer - Encoding 0 to 6 out of 27
10/12/2023 18:32:51 - INFO - Unlimiformer -   Williamson is baking,
10/12/2023 18:32:55 - INFO - Unlimiformer -   Williamson is baking,
10/12/2023 18:32:55 - INFO - Unlimiformer - Encoding 0 to 13 out of 27
10/12/2023 18:32:55 - INFO - Unlimiformer - <s><s><s><s><s><s><s>   Williamson is baking, Oppenheimer is cycling,
10/12/2023 18:32:56 - INFO - Unlimiformer - Oppenheimer is cycling,
10/12/2023 18:32:56 - INFO - Unlimiformer - Encoding 6 to 19 out of 27
10/12/2023 18:32:56 - INFO - Unlimiformer - <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>   Oppenheimer is cycling, Leechenbaum is painting,
10/12/2023 18:32:56 - INFO - Unlimiformer - Leechenbaum is painting,
10/12/2023 18:32:56 - INFO - Unlimiformer - Encoding 13 to 26 out of 27
10/12/2023 18:32:56 - INFO - Unlimiformer - <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>   Leechenbaum is painting, Zelensky is relaxing,
10/12/2023 18:32:58 - INFO - Unlimiformer - Zelensky is relaxing,
10/12/2023 18:32:58 - INFO - Unlimiformer - "Pre Forward Hook", <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>[INST] <<SYS>>
You are a helpful assistant. Answer with detailed responses according to the entire instruction or question. 
<</SYS>>

, 63
10/12/2023 18:32:58 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/contrib/torch_utils.py:44: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 2)
10/12/2023 18:33:11 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/12/2023 18:33:18 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/12/2023 18:33:26 - INFO - Unlimiformer - "Pre Forward Hook", above, 1
10/12/2023 18:33:39 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/12/2023 18:33:48 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/12/2023 18:33:56 - INFO - Unlimiformer - "Pre Forward Hook", can, 1
10/12/2023 18:34:03 - INFO - Unlimiformer - "Pre Forward Hook", you, 1
10/12/2023 18:34:10 - INFO - Unlimiformer - "Pre Forward Hook", tell, 1
10/12/2023 18:34:21 - INFO - Unlimiformer - "Pre Forward Hook", me, 1
10/12/2023 18:34:29 - INFO - Unlimiformer - "Pre Forward Hook", who, 1
10/12/2023 18:34:36 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/12/2023 18:34:42 - INFO - Unlimiformer - "Pre Forward Hook", relax, 1
10/12/2023 18:34:48 - INFO - Unlimiformer - "Pre Forward Hook", ing, 1
10/12/2023 18:34:59 - INFO - Unlimiformer - "Pre Forward Hook", ?, 1
10/12/2023 18:35:07 - INFO - Unlimiformer - "Pre Forward Hook", [, 1
10/12/2023 18:35:15 - INFO - Unlimiformer - "Pre Forward Hook", /, 1
10/12/2023 18:35:23 - INFO - Unlimiformer - "Pre Forward Hook", INST, 1
10/12/2023 18:35:30 - INFO - Unlimiformer - "Pre Forward Hook", ], 1
10/12/2023 18:35:42 - INFO - Unlimiformer - "Pre Forward Hook", , 1
10/12/2023 18:35:50 - INFO - Unlimiformer - "Pre Forward Hook", C, 1
10/12/2023 18:35:57 - INFO - Unlimiformer - "Pre Forward Hook", ertain, 1
10/12/2023 18:36:05 - INFO - Unlimiformer - "Pre Forward Hook", ly, 1
10/12/2023 18:36:14 - INFO - Unlimiformer - "Pre Forward Hook", !, 1
10/12/2023 18:36:22 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
10/12/2023 18:36:30 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/12/2023 18:36:38 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/12/2023 18:36:46 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/12/2023 18:36:54 - INFO - Unlimiformer - "Pre Forward Hook", provided, 1
10/12/2023 18:37:06 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/12/2023 18:37:13 - INFO - Unlimiformer - "Pre Forward Hook", we, 1
10/12/2023 18:37:20 - INFO - Unlimiformer - "Pre Forward Hook", can, 1
10/12/2023 18:37:28 - INFO - Unlimiformer - "Pre Forward Hook", see, 1
10/12/2023 18:37:38 - INFO - Unlimiformer - "Pre Forward Hook", that, 1
10/12/2023 18:37:48 - INFO - Unlimiformer - "Pre Forward Hook", :, 1
10/12/2023 18:37:56 - INFO - Unlimiformer - "Pre Forward Hook", 
, 1
10/12/2023 18:38:04 - INFO - Unlimiformer - "Pre Forward Hook", 
, 1
10/12/2023 18:38:13 - INFO - Unlimiformer - "Pre Forward Hook", *, 1
10/12/2023 18:38:26 - INFO - Unlimiformer - "Pre Forward Hook", William, 1
10/12/2023 18:38:34 - INFO - Unlimiformer - "Pre Forward Hook", son, 1
10/12/2023 18:38:40 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/12/2023 18:38:46 - INFO - Unlimiformer - "Pre Forward Hook", b, 1
10/12/2023 18:38:53 - INFO - Unlimiformer - "Pre Forward Hook", aking, 1
10/12/2023 18:39:02 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/12/2023 18:39:09 - INFO - Unlimiformer - "Pre Forward Hook", which, 1
10/12/2023 18:39:16 - INFO - Unlimiformer - "Pre Forward Hook", means, 1
10/12/2023 18:39:23 - INFO - Unlimiformer - "Pre Forward Hook", they, 1
10/12/2023 18:39:29 - INFO - Unlimiformer - "Pre Forward Hook", are, 1
10/12/2023 18:39:38 - INFO - Unlimiformer - "Pre Forward Hook", engaged, 1
10/12/2023 18:39:47 - INFO - Unlimiformer - "Pre Forward Hook", in, 1
10/12/2023 18:39:54 - INFO - Unlimiformer - "Pre Forward Hook", an, 1
10/12/2023 18:40:01 - INFO - Unlimiformer - "Pre Forward Hook", activity, 1
10/12/2023 18:40:08 - INFO - Unlimiformer - "Pre Forward Hook", that, 1
10/12/2023 18:40:17 - INFO - Unlimiformer - "Pre Forward Hook", requires, 1
10/12/2023 18:40:27 - INFO - Unlimiformer - "Pre Forward Hook", physical, 1
10/12/2023 18:40:35 - INFO - Unlimiformer - "Pre Forward Hook", effort, 1
10/12/2023 18:40:42 - INFO - Unlimiformer - "Pre Forward Hook", and, 1
10/12/2023 18:40:48 - INFO - Unlimiformer - "Pre Forward Hook", attention, 1
10/12/2023 18:40:58 - INFO - Unlimiformer - "Pre Forward Hook", ., 1
10/12/2023 18:41:07 - INFO - Unlimiformer - "Pre Forward Hook", 
, 1
10/12/2023 18:41:14 - INFO - Unlimiformer - "Pre Forward Hook", *, 1
10/12/2023 18:41:21 - INFO - Unlimiformer - "Pre Forward Hook", Lee, 1
10/12/2023 18:41:28 - INFO - Unlimiformer - "Pre Forward Hook", chen, 1
10/12/2023 18:41:40 - INFO - Unlimiformer - "Pre Forward Hook", baum, 1
10/12/2023 18:41:49 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/12/2023 18:41:56 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/12/2023 18:42:03 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/12/2023 18:42:11 - INFO - Unlimiformer - "Pre Forward Hook", which, 1
10/12/2023 18:42:22 - INFO - Unlimiformer - "Pre Forward Hook", also, 1
10/12/2023 18:42:31 - INFO - Unlimiformer - "Pre Forward Hook", requires, 1
10/12/2023 18:42:38 - INFO - Unlimiformer - "Pre Forward Hook", physical, 1
10/12/2023 18:42:45 - INFO - Unlimiformer - "Pre Forward Hook", effort, 1
10/12/2023 18:42:52 - INFO - Unlimiformer - "Pre Forward Hook", and, 1
10/12/2023 18:43:02 - INFO - Unlimiformer - "Pre Forward Hook", attention, 1
10/12/2023 18:43:11 - INFO - Unlimiformer - "Pre Forward Hook", ., 1
10/12/2023 18:43:18 - INFO - Unlimiformer - "Pre Forward Hook", 
, 1
10/12/2023 18:43:25 - INFO - Unlimiformer - "Pre Forward Hook", 
, 1
10/12/2023 18:43:31 - INFO - Unlimiformer - "Pre Forward Hook", There, 1
10/12/2023 18:43:42 - INFO - Unlimiformer - "Pre Forward Hook", fore, 1
10/12/2023 18:43:52 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/12/2023 18:43:58 - INFO - Unlimiformer - "Pre Forward Hook", we, 1
10/12/2023 18:44:05 - INFO - Unlimiformer - "Pre Forward Hook", can, 1
10/12/2023 18:44:12 - INFO - Unlimiformer - "Pre Forward Hook", conclude, 1
10/12/2023 18:44:20 - INFO - Unlimiformer - "Pre Forward Hook", that, 1
10/12/2023 18:44:31 - INFO - Unlimiformer - "Pre Forward Hook", neither, 1
10/12/2023 18:44:38 - INFO - Unlimiformer - "Pre Forward Hook", William, 1
10/12/2023 18:44:45 - INFO - Unlimiformer - "Pre Forward Hook", son, 1
10/12/2023 18:44:52 - INFO - Unlimiformer - "Pre Forward Hook", nor, 1
Traceback (most recent call last):
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 583, in <module>
    main()
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 534, in main
    output_sequences = model.generate(
                       ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 684, in pre_generate_hook
    vals_pred.append(self.original_generate_func(input_ids_prefix, **new_kwargs))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 1648, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 2730, in sample
    outputs = self(
              ^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 769, in pre_forward_hook
    result = self.original_forward_func(input_ids=input_ids, labels=labels, attention_mask=attention_mask, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 820, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 708, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 424, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 808, in attention_pre_forward_hook
    result = original_cross_attn_forward_func(hidden_states=hidden_states, attention_mask=attention_mask, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 321, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1547, in _call_impl
    hook_result = hook(self, args, result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 869, in attention_forward_hook
    _, top_search_key_indices = self.datastore[datastore_index].search(datastore_query, k=topk)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/index_building.py", line 34, in search
    scores, values = self.indices[i].search(queries[i], k)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/index_building.py", line 144, in search
    scores, values = faiss.knn_gpu(faiss.StandardGpuResources(), queries, self.keys, k, 
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/contrib/torch_utils.py", line 578, in torch_replacement_knn_gpu
    with using_stream(res):
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/contrib/torch_utils.py", line 83, in using_stream
    prior_stream = res.getDefaultStream(torch.cuda.current_device())
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/swigfaiss_avx2.py", line 1396, in getDefaultStream
    return _swigfaiss_avx2.StandardGpuResources_getDefaultStream(self, device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
