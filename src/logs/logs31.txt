10/15/2023 00:22:32 - WARNING - __main__ - device: cuda:7, n_gpu: 8, 16-bits training: True
Using pad_token, but it is not set yet.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:05,  2.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:05<00:02,  2.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  1.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.10s/it]
10/15/2023 00:24:22 - INFO - __main__ - Namespace(model_type='llama', model_name_or_path='meta-llama/Llama-2-13b-chat-hf', prompt='data6/original_data.txt', length=300, num_hidden_layers=None, stop_token=None, temperature=1.0, repetition_penalty=1.0, k=0, p=0, prefix='', suffix='', padding_text='', xlm_language='', seed=42, data_folder=42, no_cuda=False, stream_output=False, num_return_sequences=1, fp16=True, jit=False, device=device(type='cuda', index=7), n_gpu=8)
10/15/2023 00:24:22 - INFO - Unlimiformer - Encoding 0 to 6 out of 18
10/15/2023 00:24:22 - INFO - Unlimiformer - Williamson is baking,
10/15/2023 00:24:27 - INFO - Unlimiformer - Williamson is baking,
10/15/2023 00:24:27 - INFO - Unlimiformer - Encoding 6 to 13 out of 18
10/15/2023 00:24:27 - INFO - Unlimiformer - A, Oppenheimer is cycling,
10/15/2023 00:24:29 - INFO - Unlimiformer - Oppenheimer is cycling,
10/15/2023 00:24:29 - INFO - Unlimiformer - Encoding 13 to 18 out of 18
10/15/2023 00:24:29 - INFO - Unlimiformer - A, B, Leechenbaum is painting
10/15/2023 00:24:31 - INFO - Unlimiformer - Leechenbaum is painting
10/15/2023 00:24:31 - INFO - Unlimiformer - "Pre Forward Hook", <s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>[INST] <<SYS>>
You are a helpful assistant. Answer with short responses according to the question. 
<</SYS>>

, 52
10/15/2023 00:24:33 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/faiss/contrib/torch_utils.py:44: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  x.storage().data_ptr() + x.storage_offset() * 2)
10/15/2023 00:24:54 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/15/2023 00:25:07 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/15/2023 00:25:22 - INFO - Unlimiformer - "Pre Forward Hook", above, 1
10/15/2023 00:25:34 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/15/2023 00:25:47 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/15/2023 00:26:01 - INFO - Unlimiformer - "Pre Forward Hook", can, 1
10/15/2023 00:26:15 - INFO - Unlimiformer - "Pre Forward Hook", you, 1
10/15/2023 00:26:29 - INFO - Unlimiformer - "Pre Forward Hook", tell, 1
10/15/2023 00:26:44 - INFO - Unlimiformer - "Pre Forward Hook", me, 1
10/15/2023 00:26:57 - INFO - Unlimiformer - "Pre Forward Hook", who, 1
10/15/2023 00:27:11 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/15/2023 00:27:27 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/15/2023 00:27:40 - INFO - Unlimiformer - "Pre Forward Hook", ?, 1
10/15/2023 00:27:54 - INFO - Unlimiformer - "Pre Forward Hook", [, 1
10/15/2023 00:28:09 - INFO - Unlimiformer - "Pre Forward Hook", /, 1
10/15/2023 00:28:23 - INFO - Unlimiformer - "Pre Forward Hook", INST, 1
10/15/2023 00:28:35 - INFO - Unlimiformer - "Pre Forward Hook", ], 1
10/15/2023 00:28:50 - INFO - Unlimiformer - "Pre Forward Hook", , 1
10/15/2023 00:29:03 - INFO - Unlimiformer - "Pre Forward Hook", Sure, 1
10/15/2023 00:29:16 - INFO - Unlimiformer - "Pre Forward Hook", !, 1
10/15/2023 00:29:30 - INFO - Unlimiformer - "Pre Forward Hook", Based, 1
10/15/2023 00:29:44 - INFO - Unlimiformer - "Pre Forward Hook", on, 1
10/15/2023 00:29:58 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/15/2023 00:30:12 - INFO - Unlimiformer - "Pre Forward Hook", information, 1
10/15/2023 00:30:25 - INFO - Unlimiformer - "Pre Forward Hook", provided, 1
10/15/2023 00:30:38 - INFO - Unlimiformer - "Pre Forward Hook", ,, 1
10/15/2023 00:30:52 - INFO - Unlimiformer - "Pre Forward Hook", the, 1
10/15/2023 00:31:05 - INFO - Unlimiformer - "Pre Forward Hook", artist, 1
10/15/2023 00:31:20 - INFO - Unlimiformer - "Pre Forward Hook", painting, 1
10/15/2023 00:31:34 - INFO - Unlimiformer - "Pre Forward Hook", is, 1
10/15/2023 00:31:48 - INFO - Unlimiformer - "Pre Forward Hook", Mark, 1
10/15/2023 00:32:01 - INFO - Unlimiformer - "Pre Forward Hook", Roth, 1
Traceback (most recent call last):
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 583, in <module>
    main()
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/run_generation.py", line 534, in main
    output_sequences = model.generate(
                       ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 686, in pre_generate_hook
    vals_pred.append(self.original_generate_func(input_ids_prefix, **new_kwargs))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 1648, in generate
    return self.sample(
           ^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/generation/utils.py", line 2730, in sample
    outputs = self(
              ^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 771, in pre_forward_hook
    result = self.original_forward_func(input_ids=input_ids, labels=labels, attention_mask=attention_mask, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 820, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 708, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 424, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 817, in attention_pre_forward_hook
    result = original_cross_attn_forward_func(hidden_states=hidden_states, attention_mask=attention_mask, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 321, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/anaconda3/envs/btp/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1547, in _call_impl
    hook_result = hook(self, args, result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/unlimiformer.py", line 895, in attention_forward_hook
    _, top_search_key_indices = self.datastore[datastore_index].search(datastore_query, k=topk)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/index_building.py", line 34, in search
    scores, values = self.indices[i].search(queries[i], k)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/raid/infolab/ashwinr/RoPE/unlimiformer/src/index_building.py", line 155, in search
    values = torch.where(torch.logical_or(values < 0, values >= self.keys.shape[0]), torch.zeros_like(values), values)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
